user_path = '/data/dj'
data_path = '${user_path}/data/bca'

train = true
test = false

is_ddp = false
gpus = ['0']
n_classes = 2

label = 'Dphm'

loss_criterion = 'dice_ce'
lr = 1e-2
epochs = 1000
batch_size = 12
n_iter_per_epoch = 250
val_batch_size = 25
val_interval = 1

checkpoint_saved_interval = 50

snapshot = '${user_path}/running/bca_${label}_${loss_criterion}_epoch-${epochs}_wo-transforms'

# dataset
dataset_property_file = '${data_path}/dataset/${label}_dataset_properties.json'

#boundary_file = '${data_path}/boundaries/Skn_boundary.csv'
training_slice_txt = '${data_path}/dataset/${label}_training_slices.txt'
val_ct_txt = '${data_path}/dataset/${label}_val_cts.txt'

# for training
[slice_sample_dir]
slice_path = '${data_path}/json/slices'
image = '${slice_path}/images'
SAT = '${slice_path}/SAT'
SR = '${slice_path}/SR'
VAT = '${slice_path}/VAT'
IAM = '${slice_path}/IAM'
SMT = '${slice_path}/SMT'
SMR = '${slice_path}/SMR'
Sk = '${slice_path}/Sk'
Dphm = '${slice_path}/Dphm'


# for validation
[volume_sample_dir]
volume_path = '${data_path}/json/volume'
image = '${volume_path}/images'
SAT = '${volume_path}/SAT'
VAT = '${volume_path}/VAT'
SMT = '${volume_path}/SMT'
Sk = '${volume_path}/Sk'
Dphm = '${volume_path}/Dphm'

# Accelerator
[cudnn]
benchmark = true
deterministic = false

[network]
architecture = 'unet'
input_channels = 1
num_classes = 2
num_stages = 8
conv_dim = 2
deep_supervision = false
num_features_per_stage = [32, 64, 128, 256, 512, 512, 512, 512]
num_conv_per_stage_encoder = [2, 2, 2, 2, 2, 2, 2, 2]
num_conv_per_stage_decoder = [2, 2, 2, 2, 2, 2, 2]
kernel_sizes = [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]
strides = [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]]
conv_bias = true
non_linear_first = false
norm_op = 'InstanceNorm'
non_linear = 'leaky_relu'
[network.norm_op_kwargs]
affine = true
eps = 1e-05
[network.non_linear_kwargs]
inplace = true
